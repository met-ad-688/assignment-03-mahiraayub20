{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: Assignment 03\n",
    "author:\n",
    "  - name: Mahira Ayub\n",
    "    affiliations:\n",
    "      - id: bu\n",
    "        name: Boston University\n",
    "        city: Boston\n",
    "        state: MA\n",
    "number-sections: true\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    theme: cerulean\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "    docx: default\n",
    "    pdf: default\n",
    "date-modified: today\n",
    "date-format: long\n",
    "engine: juptyer\n",
    "embed-resourves: true\n",
    "execute:\n",
    "  echo: false\n",
    "  eval: false\n",
    "  freeze: auto\n",
    "  juptyer: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d660d0",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "The instruction below provides you with general keywords for columns used in the lightcast file. See the data schema generated after the load dataset code above to use proper column name. For each visualization, customize colors, fonts, and styles to avoid a 2.5-point deduction. Also, provide a two-sentence explanation describing key insights drawn from the graph.\n",
    "\n",
    "# Load the Raw Dataset**:\n",
    "- Use Pyspark to the `lightcast_data.csv` file into a DataFrame.\n",
    "- You can reuse the previous code.\n",
    "- [Copying code from your friend constitutes plagiarism. DO NOT DO THIS! - bloodred-bold]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae925ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/22 00:57:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m pio.renderers.default = \u001b[33m\"\u001b[39m\u001b[33mnotebook\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLightcastData\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[32m     25\u001b[39m df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33minferSchema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33mmultiLine\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33mescape\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m\"\u001b[39m).csv(\u001b[33m\"\u001b[39m\u001b[33m./data/lightcast_job_postings.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment-03-mahiraayub20/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:559\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    556\u001b[39m     sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     session = \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    561\u001b[39m     module = SparkSession._get_j_spark_session_module(session._jvm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment-03-mahiraayub20/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:635\u001b[39m, in \u001b[36mSparkSession.__init__\u001b[39m\u001b[34m(self, sparkContext, jsparkSession, options)\u001b[39m\n\u001b[32m    631\u001b[39m jSparkSessionModule = SparkSession._get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m._jvm)\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    634\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.isDefined()\n\u001b[32m    636\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass.getDefaultSession().get().sparkContext().isStopped()\n\u001b[32m    637\u001b[39m     ):\n\u001b[32m    638\u001b[39m         jsparkSession = jSparkSessionClass.getDefaultSession().get()\n\u001b[32m    639\u001b[39m         jSparkSessionModule.applyModifiableSettings(jsparkSession, options)\n",
      "\u001b[31mTypeError\u001b[39m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "#| eval: true\n",
    "#| echo: true\n",
    "#| fig-align: center\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"svg\"\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\n",
    "\n",
    "# Show Schema and Sample Data\n",
    "# print(\"--This is Diagnostic check, No need to print it in the final doc---\")\n",
    "\n",
    "# df.printSchema()   # comment this line when rendering the submission\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95787a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Start a Spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLightcastData\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the CSV file into a Spark DataFrame\u001b[39;00m\n\u001b[32m      7\u001b[39m df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33minferSchema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33mmultiLine\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m).option(\u001b[33m\"\u001b[39m\u001b[33mescape\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m\"\u001b[39m).csv(\u001b[33m\"\u001b[39m\u001b[33mlightcast_job_postings.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment-03-mahiraayub20/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:559\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    556\u001b[39m     sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     session = \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    561\u001b[39m     module = SparkSession._get_j_spark_session_module(session._jvm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/assignment-03-mahiraayub20/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:635\u001b[39m, in \u001b[36mSparkSession.__init__\u001b[39m\u001b[34m(self, sparkContext, jsparkSession, options)\u001b[39m\n\u001b[32m    631\u001b[39m jSparkSessionModule = SparkSession._get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m._jvm)\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    634\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.isDefined()\n\u001b[32m    636\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass.getDefaultSession().get().sparkContext().isStopped()\n\u001b[32m    637\u001b[39m     ):\n\u001b[32m    638\u001b[39m         jsparkSession = jSparkSessionClass.getDefaultSession().get()\n\u001b[32m    639\u001b[39m         jSparkSessionModule.applyModifiableSettings(jsparkSession, options)\n",
      "\u001b[31mTypeError\u001b[39m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a Spark DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"lightcast_job_postings.csv\")\n",
    "\n",
    "df.createOrReplaceTempView(\"jobs\")\n",
    "\n",
    "# Show schema\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
